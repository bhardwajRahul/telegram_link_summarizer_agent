###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401
# flake8: noqa: E501,F401
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\nclient<llm> Gemini2_5_flash {\n  provider google-ai\n  options {\n    model gemini-2.5-flash-preview-04-17\n    api_key env.GEMINI_API_KEY\n  }\n}\n\nclient<llm> Gemini2_5_pro {\n  provider google-ai\n  options {\n    model gemini-2.5-pro-exp-03-25\n    api_key env.GEMINI_API_KEY\n  }\n}\n\nclient<llm> DeepSeekR1 {\n  provider \"openai\"\n  options {\n    api_key env.DEEPSEEK_API_KEY\n    base_url \"https://api.deepseek.com\"\n    model \"deepseek-reasoner\"\n  }\n}\n\nclient<llm> DeepSeekV3 {\n  provider \"openai\"\n  options {\n    api_key env.DEEPSEEK_API_KEY\n    base_url \"https://api.deepseek.com\"\n    model \"deepseek-chat\"\n    temperature 0.1\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.87.2\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "router.baml": "// Define the possible extraction tools\nenum ExtractorTool {\n  WebpageExtractor // For general webpages\n  PDFExtractor     // For PDF documents\n  TwitterExtractor // For Twitter/X URLs\n  LinkedInExtractor // For LinkedIn post URLs\n  Unsupported      // For URLs or content types we cannot handle\n  YoutubeExtractor // For YouTube video URLs\n}\n\n// Define the router function\n// It takes the original message and decides which tool to use.\nfunction RouteRequest(original_message: string) -> ExtractorTool {\n  // Use a capable but fast client for routing\n//   client Gemini2_5_flash\n  client DeepSeekV3\n\n  prompt #\"\n    Analyze the following user message and determine the best tool to use for extracting content from any URL present.\n\n    User Message:\n    ---\n    {{ original_message }}\n    ---\n\n    Identify the primary URL in the message. Based *only* on the URL's structure or file extension, choose one of the following tools:\n\n    - If the URL points to a PDF file (ends with .pdf), choose PDFExtractor.\n    - If the URL is from Twitter or X (contains twitter.com or x.com), choose TwitterExtractor.\n    - If the URL is a LinkedIn post (contains linkedin.com/posts/), choose LinkedInExtractor.\n    - If the URL is a YouTube video (contains youtube.com/watch or youtu.be/), choose YoutubeExtractor.\n    - For all other standard web URLs (http or https), choose WebpageExtractor.\n    - If no URL is found, or the URL type is clearly unsupported (e.g., ftp://, mailto:), choose Unsupported.\n\n    Output *only* the name of the chosen tool from the 'ExtractorTool' enum.\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n// Optional test case\ntest RouteWebpage {\n  functions [RouteRequest]\n  args {\n    original_message #\"Summarize this blog post: https://example.com/blog/article-123\"#\n  }\n}\n\ntest RoutePDF {\n  functions [RouteRequest]\n  args {\n    original_message #\"Can you process this PDF? https://arxiv.org/pdf/2401.0001.pdf\"#\n  }\n}\n\ntest RouteTwitter {\n  functions [RouteRequest]\n  args {\n    original_message #\"Look at this thread: https://x.com/user/status/12345\"#\n  }\n}\n\ntest RouteLinkedIn {\n  functions [RouteRequest]\n  args {\n    original_message #\"Interesting thoughts here: https://www.linkedin.com/posts/someuser_activity-1234567890-abcd?utm_source=share\"#\n  }\n}\n\ntest RouteNoURL {\n  functions [RouteRequest]\n  args {\n    original_message #\"Tell me a joke.\"#\n  }\n}\n\ntest RouteUnsupported {\n  functions [RouteRequest]\n  args {\n    original_message #\"Check this out: ftp://files.example.com/data.zip\"#\n  }\n}\n\ntest RouteYouTube {\n  functions [RouteRequest]\n  args {\n    original_message #\"Summarize this video: https://www.youtube.com/watch?v=dQw4w9WgXcQ\"#\n  }\n}\n\n",
    "summarize.baml": "\n// Define an enum for the type of content being summarized\nenum ContentType {\n  Webpage\n  PDF\n  GenericText\n}\n\n// Define the structure for the summarization output\nclass Summary {\n  title string @description(\"A concise and informative title for the summarized content(max 10 words).\")\n  key_points string[] @description(\"A list of the most important points or takeaways from the content. (3-5 points)\")\n  concise_summary string @description(\"A brief paragraph summarizing the entire content. (50-100 words)\")\n}\n\n// Define the main summarization function\n// This function handles shorter texts directly or uses context for RAG-based summaries.\nfunction SummarizeContent(content: string, content_type: ContentType, context: string?) -> Summary {\n  // Specifies the client to use for the LLM call. Assumes an 'openai' client is configured in BAML settings.\n  // The model used (e.g., gpt-4-turbo-preview) is typically configured in the BAML client settings or passed via environment variables.\n  // client Gemini2_5_flash \n  client DeepSeekV3\n  // client Gemini2_5_flash\n\n  // Prompt definition using BAML's syntax\n  prompt #\"\n    You are an expert summarization engine. Your goal is to provide a clear and concise summary of the given text.\n\n    Content Type: {{ content_type }}\n    {% if context %}\n    Relevant Context (from RAG):\n    ---\n    {{ context }}\n    ---\n    {% endif %}\n\n    Original Content:\n    ---\n    {{ content }}\n    ---\n\n    Based *only* on the provided Original Content {% if context %}and the Relevant Context{% endif %}, generate the answer.\n\n    Format your response strictly as the 'Summary' class structure. Ensure the title, key points, and summary are distinct and accurately reflect the source material. \n    Do not include any information not present in the provided text or context.\n\n    # Instructions\n    - If the long is for a paper, you need to explain what the paper is trying to solve and how, in separate sections: \n      '## What is the problem the paper is trying to solve?'\n      '## How does the paper attempt to solve the problem?'\n    - If it's a blog post or webpage, you have to explain like: 'This post or blog or webpage is about ...'\n    - If it's a github repo, you have to explain like: 'This github repo is about ... and tries to solve .... It uses ...'\n    - If it's an arxive or any other paper, do not mention info about DIO or under process or stuff like that. Just mentione the main points about the paper.\n    \n    ----\n    {{ ctx.output_format}}\n  \"#\n}\n\ntest SummarizeTest {\n  functions [SummarizeContent]\n  args {\n    content #\"\n      The Urgency of Interpretability\nApril 2025\nIn the decade that I have been working on AI, I’ve watched it grow from a tiny academic field to arguably the most important economic and geopolitical issue in the world.  In all that time, perhaps the most important lesson I’ve learned is this: the progress of the underlying technology is inexorable, driven by forces too powerful to stop, but the way in which it happens—the order in which things are built, the applications we choose, and the details of how it is rolled out to society—are eminently possible to change, and it’s possible to have great positive impact by doing so.  We can’t stop the bus, but we can steer it.  In the past I’ve written about the importance of deploying AI in a way that is positive for the world, and of ensuring that democracies build and wield the technology before autocracies do.  Over the last few months, I have become increasingly focused on an additional opportunity for steering the bus: the tantalizing possibility, opened up by some recent advances, that we could succeed at interpretability—that is, in understanding the inner workings of AI systems—before models reach an overwhelming level of power.\n    \"#\n    content_type #\"Webpage\"#\n  }\n}\n",
}

def get_baml_files():
    return file_map